{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week Seven: Sentiment with GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll convert the RNN sentiment classifier from last time into a **GRU** RNN sentiment classifier. While the small dataset and tiny vocabulary that we're using here (for speed) will limit the performance of the model, it should still do substantially better than the plain RNN.\n",
    "\n",
    "![](http://vignette1.wikia.nocookie.net/despicableme/images/b/ba/Gru.jpg/revision/latest/scale-to-width-down/250?cb=20130711023954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors.\n",
    "\n",
    "To simplify your implementation, we'll use a fixed unrolling length of 20. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "\n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 20\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = SEQ_LEN - len(token_sequence)\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i >= padding:\n",
    "                    if token_sequence[i - padding] in word_indices:\n",
    "                        index = word_indices[token_sequence[i - padding]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Pretty good little movie .', 'index_sequence': array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0, 195, 351, 487, 732, 819], dtype=int32), 'label': 1}\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print training_set[18]\n",
    "print len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the class below to convert it into a GRU model. You should have to:\n",
    "\n",
    "- Add additional trained parameters.\n",
    "- Modify the `step()` function.\n",
    "- Modify L2 regularization to incorporate the new parameters.\n",
    "\n",
    "You shouldn't have to edit anything outside of `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNSentimentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 1.0  # Should be about right\n",
    "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
    "        self.dim = 12  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.l2_lambda = 0.001\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        self.W_r = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_r = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        self.W_z = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_z = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define one step of the RNN\n",
    "        def step(x, h_prev):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_h_prev = tf.concat(1, [emb, h_prev])\n",
    "            z = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_z)  + self.b_z)\n",
    "            r = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_r)  + self.b_r)\n",
    "            emb_r_h_prev = tf.concat(1, [emb, r * h_prev])\n",
    "            h_tilde = tf.nn.tanh(tf.matmul(emb_r_h_prev, self.W_rnn)  + self.b_rnn)\n",
    "            h = (1. - z) * h_prev + z * h_tilde\n",
    "            return h\n",
    "\n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_slices = tf.split(1, self.sequence_length, self.x)\n",
    "        \n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        \n",
    "        # Unroll the RNN\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            h_prev = step(x_t, h_prev)\n",
    "        \n",
    "        # Compute the logits using one last linear layer\n",
    "        self.logits = tf.matmul(h_prev, self.W_cl) + self.b_cl\n",
    "\n",
    "        # Define the L2 cost\n",
    "        self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_z)) + \n",
    "                                         tf.reduce_sum(tf.square(self.W_r)) + \n",
    "                                         tf.reduce_sum(tf.square(self.W_cl)))\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, self.y)) \\\n",
    "            + self.l2_cost\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256])  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. If the GRU is doing what it should, you should reach 74% accuracy within your first 200 epochsâ€”a substantial improvement over the 70% figure we saw last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 5 Cost: 0.00272594029256 Dev acc: 0.515625 Train acc: 0.578125\n",
      "Epoch: 10 Cost: 0.00271705439728 Dev acc: 0.484375 Train acc: 0.52734375\n",
      "Epoch: 15 Cost: 0.0027131375391 Dev acc: 0.515625 Train acc: 0.48828125\n",
      "Epoch: 20 Cost: 0.00270917687427 Dev acc: 0.515625 Train acc: 0.53515625\n",
      "Epoch: 25 Cost: 0.00271003589 Dev acc: 0.515625 Train acc: 0.5078125\n",
      "Epoch: 30 Cost: 0.00270958790659 Dev acc: 0.515625 Train acc: 0.51171875\n",
      "Epoch: 35 Cost: 0.00270593757914 Dev acc: 0.515625 Train acc: 0.49609375\n",
      "Epoch: 40 Cost: 0.00270748297098 Dev acc: 0.515625 Train acc: 0.5078125\n",
      "Epoch: 45 Cost: 0.00270647911734 Dev acc: 0.51953125 Train acc: 0.5390625\n",
      "Epoch: 50 Cost: 0.00270740824959 Dev acc: 0.515625 Train acc: 0.5390625\n",
      "Epoch: 55 Cost: 0.0027074857132 Dev acc: 0.5078125 Train acc: 0.625\n",
      "Epoch: 60 Cost: 0.00269979731766 Dev acc: 0.5234375 Train acc: 0.5234375\n",
      "Epoch: 65 Cost: 0.00270135373877 Dev acc: 0.50390625 Train acc: 0.5859375\n",
      "Epoch: 70 Cost: 0.00268661305826 Dev acc: 0.50390625 Train acc: 0.6171875\n",
      "Epoch: 75 Cost: 0.00267773125045 Dev acc: 0.546875 Train acc: 0.59375\n",
      "Epoch: 80 Cost: 0.00263526455361 Dev acc: 0.53515625 Train acc: 0.56640625\n",
      "Epoch: 85 Cost: 0.00265249792762 Dev acc: 0.57421875 Train acc: 0.703125\n",
      "Epoch: 90 Cost: 0.00262322888227 Dev acc: 0.59765625 Train acc: 0.62109375\n",
      "Epoch: 95 Cost: 0.00262189553016 Dev acc: 0.578125 Train acc: 0.60546875\n",
      "Epoch: 100 Cost: 0.00257928659967 Dev acc: 0.6328125 Train acc: 0.640625\n",
      "Epoch: 105 Cost: 0.00236749610243 Dev acc: 0.67578125 Train acc: 0.73828125\n",
      "Epoch: 110 Cost: 0.00223510413155 Dev acc: 0.69921875 Train acc: 0.73828125\n",
      "Epoch: 115 Cost: 0.00208750753491 Dev acc: 0.7265625 Train acc: 0.76171875\n",
      "Epoch: 120 Cost: 0.00189158861543 Dev acc: 0.7421875 Train acc: 0.83203125\n",
      "Epoch: 125 Cost: 0.00181471846601 Dev acc: 0.74609375 Train acc: 0.80859375\n",
      "Epoch: 130 Cost: 0.00179289128304 Dev acc: 0.75 Train acc: 0.83984375\n",
      "Epoch: 135 Cost: 0.00169899051198 Dev acc: 0.73046875 Train acc: 0.82421875\n",
      "Epoch: 140 Cost: 0.00164823262107 Dev acc: 0.765625 Train acc: 0.8515625\n",
      "Epoch: 145 Cost: 0.00160096896623 Dev acc: 0.7578125 Train acc: 0.8359375\n",
      "Epoch: 150 Cost: 0.00157667412768 Dev acc: 0.75 Train acc: 0.8515625\n",
      "Epoch: 155 Cost: 0.00155794082848 Dev acc: 0.75390625 Train acc: 0.80859375\n",
      "Epoch: 160 Cost: 0.00150597808641 Dev acc: 0.74609375 Train acc: 0.890625\n",
      "Epoch: 165 Cost: 0.00148490184263 Dev acc: 0.734375 Train acc: 0.875\n",
      "Epoch: 170 Cost: 0.00147306886329 Dev acc: 0.734375 Train acc: 0.8515625\n",
      "Epoch: 175 Cost: 0.0014283885981 Dev acc: 0.7421875 Train acc: 0.8828125\n",
      "Epoch: 180 Cost: 0.0014050363356 Dev acc: 0.73046875 Train acc: 0.828125\n",
      "Epoch: 185 Cost: 0.00138625892793 Dev acc: 0.7265625 Train acc: 0.8671875\n",
      "Epoch: 190 Cost: 0.00138526757817 Dev acc: 0.75 Train acc: 0.88671875\n",
      "Epoch: 195 Cost: 0.0013144406559 Dev acc: 0.73828125 Train acc: 0.89453125\n",
      "Epoch: 200 Cost: 0.00136177511282 Dev acc: 0.74609375 Train acc: 0.91796875\n",
      "Epoch: 205 Cost: 0.001315537866 Dev acc: 0.7265625 Train acc: 0.875\n",
      "Epoch: 210 Cost: 0.0012364867014 Dev acc: 0.7421875 Train acc: 0.90625\n",
      "Epoch: 215 Cost: 0.00115083477727 Dev acc: 0.734375 Train acc: 0.92578125\n",
      "Epoch: 220 Cost: 0.00124280157292 Dev acc: 0.73046875 Train acc: 0.90234375\n",
      "Epoch: 225 Cost: 0.00114819809743 Dev acc: 0.73828125 Train acc: 0.91015625\n",
      "Epoch: 230 Cost: 0.00109280889026 Dev acc: 0.7421875 Train acc: 0.9453125\n",
      "Epoch: 235 Cost: 0.00115255627962 Dev acc: 0.734375 Train acc: 0.86328125\n",
      "Epoch: 240 Cost: 0.0010752709134 Dev acc: 0.73828125 Train acc: 0.91015625\n",
      "Epoch: 245 Cost: 0.00108329685733 Dev acc: 0.7265625 Train acc: 0.921875\n",
      "Epoch: 250 Cost: 0.0010597309102 Dev acc: 0.72265625 Train acc: 0.9375\n",
      "Epoch: 255 Cost: 0.00107173421377 Dev acc: 0.734375 Train acc: 0.9296875\n",
      "Epoch: 260 Cost: 0.00109715157628 Dev acc: 0.7265625 Train acc: 0.9140625\n",
      "Epoch: 265 Cost: 0.00105148379872 Dev acc: 0.73828125 Train acc: 0.9296875\n",
      "Epoch: 270 Cost: 0.00101479460028 Dev acc: 0.73828125 Train acc: 0.92578125\n",
      "Epoch: 275 Cost: 0.00104059966247 Dev acc: 0.72265625 Train acc: 0.8984375\n",
      "Epoch: 280 Cost: 0.000965007442843 Dev acc: 0.71875 Train acc: 0.953125\n",
      "Epoch: 285 Cost: 0.000909870410666 Dev acc: 0.73046875 Train acc: 0.9453125\n",
      "Epoch: 290 Cost: 0.000964978032883 Dev acc: 0.734375 Train acc: 0.94140625\n",
      "Epoch: 295 Cost: 0.000902554358752 Dev acc: 0.72265625 Train acc: 0.92578125\n",
      "Epoch: 300 Cost: 0.00102256509881 Dev acc: 0.71875 Train acc: 0.9140625\n",
      "Epoch: 305 Cost: 0.00108373719397 Dev acc: 0.7265625 Train acc: 0.9296875\n",
      "Epoch: 310 Cost: 0.000888997343955 Dev acc: 0.7109375 Train acc: 0.9609375\n",
      "Epoch: 315 Cost: 0.000942215000072 Dev acc: 0.70703125 Train acc: 0.87890625\n",
      "Epoch: 320 Cost: 0.000858992648621 Dev acc: 0.73046875 Train acc: 0.95703125\n",
      "Epoch: 325 Cost: 0.000888800427438 Dev acc: 0.71875 Train acc: 0.95703125\n",
      "Epoch: 330 Cost: 0.000797183216222 Dev acc: 0.7265625 Train acc: 0.94140625\n",
      "Epoch: 335 Cost: 0.000906547723355 Dev acc: 0.7265625 Train acc: 0.953125\n",
      "Epoch: 340 Cost: 0.000791885085939 Dev acc: 0.72265625 Train acc: 0.97265625\n",
      "Epoch: 345 Cost: 0.001134964396 Dev acc: 0.73046875 Train acc: 0.96484375\n",
      "Epoch: 350 Cost: 0.000758322887123 Dev acc: 0.71875 Train acc: 0.9375\n",
      "Epoch: 355 Cost: 0.000853540093414 Dev acc: 0.7265625 Train acc: 0.95703125\n",
      "Epoch: 360 Cost: 0.000992265920138 Dev acc: 0.7265625 Train acc: 0.91796875\n",
      "Epoch: 365 Cost: 0.000828385346621 Dev acc: 0.73046875 Train acc: 0.8671875\n",
      "Epoch: 370 Cost: 0.000757718495421 Dev acc: 0.7265625 Train acc: 0.96875\n",
      "Epoch: 375 Cost: 0.000973364357681 Dev acc: 0.72265625 Train acc: 0.94921875\n",
      "Epoch: 380 Cost: 0.000744464869732 Dev acc: 0.734375 Train acc: 0.97265625\n",
      "Epoch: 385 Cost: 0.000731418584042 Dev acc: 0.73828125 Train acc: 0.96484375\n",
      "Epoch: 390 Cost: 0.000735739241699 Dev acc: 0.70703125 Train acc: 0.96875\n",
      "Epoch: 395 Cost: 0.000812487540922 Dev acc: 0.734375 Train acc: 0.9609375\n",
      "Epoch: 400 Cost: 0.000835076886384 Dev acc: 0.734375 Train acc: 0.9375\n",
      "Epoch: 405 Cost: 0.000670827741959 Dev acc: 0.7421875 Train acc: 0.94921875\n",
      "Epoch: 410 Cost: 0.000673586545788 Dev acc: 0.73046875 Train acc: 0.94921875\n",
      "Epoch: 415 Cost: 0.000665551027783 Dev acc: 0.73828125 Train acc: 0.96875\n",
      "Epoch: 420 Cost: 0.00107679309116 Dev acc: 0.74609375 Train acc: 0.9765625\n",
      "Epoch: 425 Cost: 0.000828936344651 Dev acc: 0.75 Train acc: 0.94921875\n",
      "Epoch: 430 Cost: 0.000693744382631 Dev acc: 0.75 Train acc: 0.98046875\n",
      "Epoch: 435 Cost: 0.000641619510673 Dev acc: 0.72265625 Train acc: 0.9765625\n",
      "Epoch: 440 Cost: 0.000664749631175 Dev acc: 0.73828125 Train acc: 0.9765625\n",
      "Epoch: 445 Cost: 0.000623472903734 Dev acc: 0.72265625 Train acc: 0.9609375\n",
      "Epoch: 450 Cost: 0.000905117347699 Dev acc: 0.73828125 Train acc: 0.96484375\n",
      "Epoch: 455 Cost: 0.000988285341817 Dev acc: 0.7265625 Train acc: 0.96875\n",
      "Epoch: 460 Cost: 0.000621073713302 Dev acc: 0.71875 Train acc: 0.97265625\n",
      "Epoch: 465 Cost: 0.000591872102598 Dev acc: 0.72265625 Train acc: 0.984375\n",
      "Epoch: 470 Cost: 0.000638298336761 Dev acc: 0.73046875 Train acc: 0.9765625\n",
      "Epoch: 475 Cost: 0.00074162024194 Dev acc: 0.73046875 Train acc: 0.95703125\n",
      "Epoch: 480 Cost: 0.000906565600653 Dev acc: 0.73046875 Train acc: 0.9765625\n",
      "Epoch: 485 Cost: 0.000603885063255 Dev acc: 0.71484375 Train acc: 0.98046875\n",
      "Epoch: 490 Cost: 0.00111351151847 Dev acc: 0.7109375 Train acc: 0.94921875\n",
      "Epoch: 495 Cost: 0.000570475842795 Dev acc: 0.703125 Train acc: 0.984375\n",
      "Epoch: 500 Cost: 0.000629621399437 Dev acc: 0.72265625 Train acc: 0.9609375\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus: Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you finish early, try converting the GRU RNN to a *deep* GRU RNN with two 12-dimensional recurrent layers. This will require still more parameters, a more complex step function, and a few other miscellaneous changes within `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
